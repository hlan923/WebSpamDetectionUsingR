# -*- coding: utf-8 -*-
###Project Web Spam.R

#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/1nuCKZ717h3zsucLZeABQwz3npL3b7veb

##WEB SPAM DETECTION PROJECT

###Merging hostnames, labels, and assessments files
###
    
#install.packages('dplyr')
library(dplyr)

# Read data from text files into dataframes
column_headers <- c("hostid", "hostname")
file1 <- read.table('WEBSPAM-UK2007-hostnames.txt', header = TRUE, col.names = column_headers)

column_headers <- c("hostid", "label", "spamicity", "assessments")
file2s1 <- read.table('WEBSPAM-UK2007-SET1-labels.txt', header = TRUE, col.names = column_headers)
file2s2 <- read.table('WEBSPAM-UK2007-SET2-labels.txt', header = TRUE, col.names = column_headers)
file2 <- rbind(file2s1, file2s2)

column_headers <- c("hostid", "accessor", "label", "timestamp", "period")
file3s1 <- read.table('WEBSPAM-UK2007-SET1-raw-assessments.txt', header = TRUE, col.names = column_headers)
file3s2 <- read.table('WEBSPAM-UK2007-SET2-raw-assessments.txt', header = TRUE, col.names = column_headers)
file3 <- rbind(file3s1, file3s2)

# Merge dataframes file1 and file2, and then merge the result with file3
file12 <- full_join(file1, file2, by = "hostid")
file <- full_join(file12, file3, by = "hostid")

# Removing specified columns since they will not add values for classification
file <- subset(file, select = -c(spamicity, assessments, accessor, timestamp, period))

# Merge label.x and label.y into a single label column
file <- file %>% mutate(label = case_when(is.na(label.x) | label.x == "undecided" ~ ifelse(is.na(label.y) | label.y %in% c("-", "unknown"), NA, label.y),TRUE ~ label.x)) %>% select(-label.x, -label.y) %>% filter(!is.na(label) & label != "borderline")

head(file)
print(nrow(file))

######Clean "*Obvious Features*" Dataset

###

obvious_file<- read.csv('uk-2007-05.obvious_features.csv')

#Changing column name X.hostid to hostid
names(obvious_file)[names(obvious_file) == "X.hostid"] <- "hostid"

head(obvious_file)
print(nrow(obvious_file))

obvious_merged_file <- full_join(obvious_file, file, by = "hostid")

#Removing column hostname.y since it is the same with hostname.x
obvious_merged_file <- obvious_merged_file[, -which(names(obvious_merged_file) == "hostname.y")]

#Changing column name hostname.x to hostname
names(obvious_merged_file)[names(obvious_merged_file) == "hostname.x"] <- "hostname"

#Removing specified columns since they will not add values to the classification
columns_to_remove <- c("hostid","hostname")
obvious_merged_file <- obvious_merged_file[, -which(names(obvious_merged_file) %in% columns_to_remove)]

#Removing the rows that have null value for column label since they will not add values for classification
obvious_merged_file <- subset(obvious_merged_file, !is.na(label))

head(obvious_merged_file)
print(nrow(obvious_merged_file))

#Count the number of rows with NA values

na_rows <- nrow(obvious_merged_file) - sum(complete.cases(obvious_merged_file))
print(na_rows)

#Get the unique categories of the column label

label_categories <- unique(obvious_merged_file$label)
print(label_categories)

#Assign 0 to nonspam and 1 to spam in column label
obvious_merged_file$label <- ifelse(obvious_merged_file$label == "spam", 1, 0)

head(obvious_merged_file)
print(nrow(obvious_merged_file))

# Calculating count of each value in the column label
value_counts <- table(obvious_merged_file$label)
print(value_counts)

#install.packages("ROSE")
library(ROSE)

# Balance the data with oversampling
balanced <- ovun.sample(label~., data=obvious_merged_file, p=0.5, seed=1, method="over")$data

table(balanced$label)

obvious_merged_file = balanced
head(obvious_merged_file)
print(nrow(obvious_merged_file))

#Columns to scale (excluding the last column)
cols_to_scale <- 1:(ncol(obvious_merged_file) - 1)

# Scale the selected columns
scaled_obvious <- obvious_merged_file
scaled_obvious[, cols_to_scale] <- scale(obvious_merged_file[, cols_to_scale])

obvious_merged_file = scaled_obvious

head(obvious_merged_file)
print(nrow(obvious_merged_file))

######Train classification models for "*Obvious Features*"
###

#install.packages('caret')
library(caret)

#Splitting the dataset into the training set and testing set using stratified sampling
set.seed(123)
trainIndex <- createDataPartition(obvious_merged_file$label, p = 0.7, list = FALSE, times = 1)

obvious_train <- obvious_merged_file[trainIndex, ]
obvious_test <- obvious_merged_file[-trainIndex, ]

###Decision Tree###

#install.packages("rpart")
library(rpart)

#Train Decision Tree model
obvious_dt <- rpart(label ~ ., data = obvious_train, method = "class")

#Make predictions on test data
predictions <- predict(obvious_dt, newdata = obvious_test, type = "prob")[, 2]

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == obvious_test$label)
cat("Accuracy:", accuracy, "\n")

#install.packages("pROC")
library(pROC)

#Generate ROC curve
roc_obj <- roc(obvious_test$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

#Plot ROC curve
plot(roc_obj, main = "ROC Curve for Decision Tree", col = "blue")

###Random Forest###

#install.packages("randomForest")
library(randomForest)

#Train Random Forest model
obvious_rf <- randomForest(factor(label) ~ ., data = obvious_train, ntree = 100)

#Make predictions on test data
predictions <- predict(obvious_rf, newdata = obvious_test, type = "prob")[, 2]

# Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == obvious_test$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(obvious_test$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Random Forest", col = "blue")

###Naive Bayes###

train_data = obvious_train
test_data = obvious_test

#install.packages("e1071")
library(e1071)

#Train Naive Bayes model
nb_model <- naiveBayes(label ~ ., data = train_data)

#Make predictions on test data
predictions <- predict(nb_model, newdata = test_data, type = "raw")

#Calculate accuracy
predicted_labels <- ifelse(predictions[,2] >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == test_data$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(test_data$label == 1, TRUE, FALSE), predictions[,2])

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Naive Bayes", col = "blue")

###Logistic Regression###

# Train Logistic Regression model
lr <- glm(label ~ ., data = train_data, family = binomial)

#Make predictions on test data
predictions <- predict(lr, newdata = test_data, type = "response")

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == test_data$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(test_data$label == 1, TRUE, FALSE), predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Logistic Regression", col = "blue")

######Clean "*Link Based Features*" Dataset



###

# Read link transformed file
link_file <- read.csv('uk-2007-05.link_based_features.csv')

#Changing column name X.hostid to hostid
names(link_file)[names(link_file) == "X.hostid"] <- "hostid"

head(link_file)
print(nrow(link_file))

merged_link_file <- full_join(link_file, file, by = "hostid")

#Removing column hostname.y since it is the same with hostname.x
merged_link_file <- merged_link_file[, -which(names(merged_link_file) == "hostname.y")]

#Changing column name hostname.x to hostname
names(merged_link_file)[names(merged_link_file) == "hostname.x"] <- "hostname"

#Removing specified columns since they will not add values to the classification
columns_to_remove <- c("hostid","hostname")
merged_link_file <- merged_link_file[, -which(names(merged_link_file) %in% columns_to_remove)]

#Removing the rows that have null value for column label since they will not add values for classification
merged_link_file <- subset(merged_link_file, !is.na(label))

head(merged_link_file)
print(nrow(merged_link_file))

#Count the number of rows with NA values

na_rows <- nrow(merged_link_file) - sum(complete.cases(merged_link_file))
print(na_rows)

#Get the unique categories of the column label

label_categories <- unique(merged_link_file$label)
print(label_categories)

#Assign 0 to nonspam and 1 to spam in column label
merged_link_file$label <- ifelse(merged_link_file$label == "spam", 1, 0)

head(merged_link_file)
print(nrow(merged_link_file))

# Calculating count of each value in the column label
value_counts <- table(merged_link_file$label)
print(value_counts)

# Balance the data with oversampling
balanced <- ovun.sample(label~., data=merged_link_file, p=0.5, seed=1, method="over")$data

table(balanced$label)

merged_link_file = balanced
head(merged_link_file)
print(nrow(merged_link_file))

#Columns to scale (excluding the last column)
cols_to_scale <- 1:(ncol(merged_link_file) - 1)

# Scale the selected columns
scaled_obvious <- merged_link_file
scaled_obvious[, cols_to_scale] <- scale(merged_link_file[, cols_to_scale])

merged_link_file = scaled_obvious

head(merged_link_file)
print(nrow(merged_link_file))

######Train classification models for "*Link Based Features*"
###

#Splitting the dataset into the training set and testing set using stratified sampling
set.seed(123)
trainIndex <- createDataPartition(merged_link_file$label, p = 0.7, list = FALSE, times = 1)

trainSet <- merged_link_file[trainIndex, ]
testSet <- merged_link_file[-trainIndex, ]

###Decision Tree###

#Train Decision Tree model
dt <- rpart(label ~ ., data = trainSet, method = "class")

#Make predictions on test data
predictions <- predict(dt, newdata = testSet, type = "prob")[, 2]

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

#Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

#Plot ROC curve
plot(roc_obj, main = "ROC Curve for Decision Tree", col = "blue")

###Random Forest###

#Train Random Forest model
rf <- randomForest(factor(label) ~ ., data = trainSet, ntree = 100)

#Make predictions on test data
predictions <- predict(rf, newdata = testSet, type = "prob")[, 2]

# Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Random Forest", col = "blue")

###Naive Bayes###

#Train Naive Bayes model
nb_model <- naiveBayes(label ~ ., data = trainSet)

#Make predictions on test data
predictions <- predict(nb_model, newdata = testSet, type = "raw")

#Calculate accuracy
predicted_labels <- ifelse(predictions[,2] >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions[,2])

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Naive Bayes", col = "blue")

###Logistic Regression###

# Train Logistic Regression model
lr <- glm(label ~ ., data = trainSet, family = binomial)

#Make predictions on test data
predictions <- predict(lr, newdata = testSet, type = "response")

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Logistic Regression", col = "blue")

######Clean "*Link Based Features TRANSFORMED*" Dataset

###

# Read link transformed file
linkTransformed_file <- read.csv('uk-2007-05.link_based_features_transformed.csv')

#Changing column name X.hostid to hostid
names(linkTransformed_file)[names(linkTransformed_file) == "X.hostid"] <- "hostid"

head(linkTransformed_file)
print(nrow(linkTransformed_file))

merged_linkTransformed_file <- full_join(linkTransformed_file, file, by = "hostid")

#Removing specified columns since they will not add values to the classification
columns_to_remove <- c("hostid","hostname")
merged_linkTransformed_file <- merged_linkTransformed_file[, -which(names(merged_linkTransformed_file) %in% columns_to_remove)]

#Removing the rows that have null value for column label since they will not add values for classification
merged_linkTransformed_file <- subset(merged_linkTransformed_file, !is.na(label))

head(merged_linkTransformed_file)
print(nrow(merged_linkTransformed_file))

#Count the number of rows with NA values

na_rows <- nrow(merged_linkTransformed_file) - sum(complete.cases(merged_linkTransformed_file))
print(na_rows)

#Get the unique categories of the column label

label_categories <- unique(merged_linkTransformed_file$label)
print(label_categories)

#Assign 0 to nonspam and 1 to spam in column label
merged_linkTransformed_file$label <- ifelse(merged_linkTransformed_file$label == "spam", 1, 0)

head(merged_linkTransformed_file)
print(nrow(merged_linkTransformed_file))

# Calculating count of each value in the column label
value_counts <- table(merged_linkTransformed_file$label)
print(value_counts)

# Balance the data with oversampling
balanced <- ovun.sample(label~., data=merged_linkTransformed_file, p=0.5, seed=1, method="over")$data

table(balanced$label)

merged_linkTransformed_file = balanced
head(merged_linkTransformed_file)
print(nrow(merged_linkTransformed_file))

#Columns to scale (excluding the last column)
cols_to_scale <- 1:(ncol(merged_linkTransformed_file) - 1)

# Scale the selected columns
scaled_obvious <- merged_linkTransformed_file
scaled_obvious[, cols_to_scale] <- scale(merged_linkTransformed_file[, cols_to_scale])

merged_linkTransformed_file = scaled_obvious

head(merged_linkTransformed_file)
print(nrow(merged_linkTransformed_file))

######Train classification models for "*Link Based Features TRANSFORMED*"
###

#Splitting the dataset into the training set and testing set using stratified sampling
set.seed(123)
trainIndex <- createDataPartition(merged_linkTransformed_file$label, p = 0.7, list = FALSE, times = 1)

trainSet <- merged_linkTransformed_file[trainIndex, ]
testSet <- merged_linkTransformed_file[-trainIndex, ]

###Decision Tree###

#Train Decision Tree model
dt <- rpart(label ~ ., data = trainSet, method = "class")

#Make predictions on test data
predictions <- predict(dt, newdata = testSet, type = "prob")[, 2]

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

#Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

#Plot ROC curve
plot(roc_obj, main = "ROC Curve for Decision Tree", col = "blue")

###Random Forest###

#Train Random Forest model
rf <- randomForest(factor(label) ~ ., data = trainSet, ntree = 100)

#Make predictions on test data
predictions <- predict(rf, newdata = testSet, type = "prob")[, 2]

# Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Random Forest", col = "blue")

###Naive Bayes###

#Train Naive Bayes model
nb_model <- naiveBayes(label ~ ., data = trainSet)

#Make predictions on test data
predictions <- predict(nb_model, newdata = testSet, type = "raw")

#Calculate accuracy
predicted_labels <- ifelse(predictions[,2] >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions[,2])

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Naive Bayes", col = "blue")

###Logistic Regression###

# Train Logistic Regression model
lr <- glm(label ~ ., data = trainSet, family = binomial)

#Make predictions on test data
predictions <- predict(lr, newdata = testSet, type = "response")

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Logistic Regression", col = "blue")

######Clean "*Content Based Features*" Dataset###

# ==========================================================================================================
# Content based features dataset merge with file
# ==========================================================================================================

# Read content based features dataset
contentFile <- read.csv('uk-2007-05.content_based_features.csv')
names(contentFile)[names(contentFile) == "X.hostid"] <- "hostid"

head(contentFile)
print(nrow(contentFile))

# Merge content file with previously merged file
mergedContentFile <- full_join(contentFile, file, by = "hostid")

# Combining columns, hostname.y and hostname.x to become hostname
mergedContentFile <- mergedContentFile[, -which(names(mergedContentFile) == "hostname.y")]
names(mergedContentFile)[names(mergedContentFile) == "hostname.x"] <- "hostname"

#Removing specified columns since they will not add values to the classification
mergedContentFile <- subset(mergedContentFile, select = -c(hostid, hostname))

# Remove rows with NA values
mergedContentFile <- na.omit(mergedContentFile)

head(mergedContentFile)
print(nrow(mergedContentFile))

#Get the unique categories of the column label

label_categories <- unique(mergedContentFile$label)
print(label_categories)

#Assign 0 to nonspam and 1 to spam in column label
mergedContentFile$label <- ifelse(mergedContentFile$label == "spam", 1, 0)

head(mergedContentFile)
print(nrow(mergedContentFile))

# Calculating count of each value in the column label
value_counts <- table(mergedContentFile$label)
print(value_counts)

# Balance the data with oversampling
balanced <- ovun.sample(label~., data=mergedContentFile, p=0.5, seed=1, method="over")$data
table(balanced$label)

mergedContentFile = balanced

head(mergedContentFile)
print(nrow(mergedContentFile))

#Columns to scale (excluding the last column)
cols_to_scale <- 1:(ncol(mergedContentFile) - 1)

# Scale the selected columns
scaled <- mergedContentFile
scaled[, cols_to_scale] <- scale(mergedContentFile[, cols_to_scale])

mergedContentFile = scaled

head(mergedContentFile)
print(nrow(mergedContentFile))

######Train classification models for "*Content Based Features*"
###

# Split the data into training set and testing set
set.seed(123)
train_indices <- createDataPartition(y = mergedContentFile$label, p = 0.7, list = FALSE, times = 1)
trainSet <- mergedContentFile[train_indices, ]
testSet <- mergedContentFile[-train_indices, ]

###Decision Tree###

#Train Decision Tree model
dt <- rpart(label ~ ., data = trainSet, method = "class")

#Make predictions on test data
predictions <- predict(dt, newdata = testSet, type = "prob")[, 2]

# Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Decision Tree", col = "blue")

###Random Forest###

# Train Random Forest model
rf_model <- randomForest(factor(label) ~ ., data = trainSet, ntree = 100)

# Make predictions on test data
predictions <- predict(rf_model, newdata = testSet, type = "prob")[, 2]

# Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Random Forest", col = "blue")

###Naive Bayes

###

#Train Naive Bayes model
nb_model <- naiveBayes(label ~ ., data = trainSet)

#Make predictions on test data
predictions <- predict(nb_model, newdata = testSet, type = "raw")

#Calculate accuracy
predicted_labels <- ifelse(predictions[,2] >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions[,2])

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Naive Bayes", col = "blue")

###Logistic regression###

# Train Logistic Regression model
lr <- glm(label ~ ., data = trainSet, family = binomial)

#Make predictions on test data
predictions <- predict(lr, newdata = testSet, type = "response")

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Logistic Regression", col = "blue")

######Clean "*Combined Features*" Dataset###

obvious_merged_file <- full_join(obvious_file, file, by = "hostid")

#Removing column hostname.y since it is the same with hostname.x
obvious_merged_file <- obvious_merged_file[, -which(names(obvious_merged_file) == "hostname.y")]

#Changing column name hostname.x to hostname
names(obvious_merged_file)[names(obvious_merged_file) == "hostname.x"] <- "hostname"

#Removing the rows that have null value for column label since they will not add values for classification
obvious_merged_file <- subset(obvious_merged_file, !is.na(label))

head(obvious_merged_file)
print(nrow(obvious_merged_file))

merged_linkTransformed_file <- full_join(linkTransformed_file, obvious_merged_file, by = "hostid")

#Removing the rows that have null value for column label since they will not add values for classification
merged_linkTransformed_file <- subset(merged_linkTransformed_file, !is.na(label))

head(merged_linkTransformed_file)
print(nrow(merged_linkTransformed_file))

mergedContentFile <- full_join(contentFile, merged_linkTransformed_file, by = "hostid")

#Removing specified columns since they will not add values to the classification
mergedContentFile <- subset(mergedContentFile, select = -c(hostid, hostname.x, hostname.y))

#Removing rows with NA values
mergedContentFile <- na.omit(mergedContentFile)

combinedFile = mergedContentFile

head(combinedFile)
print(nrow(combinedFile))

#Get the unique categories of the column label

label_categories <- unique(combinedFile$label)
print(label_categories)

#Assign 0 to nonspam and 1 to spam in column label
combinedFile$label <- ifelse(combinedFile$label == "spam", 1, 0)

head(combinedFile)
print(nrow(combinedFile))

# Calculating count of each value in the column label
value_counts <- table(combinedFile$label)
print(value_counts)

# Balance the data with oversampling
balanced <- ovun.sample(label~., data=combinedFile, p=0.5, seed=1, method="over")$data
table(balanced$label)

combinedFile = balanced

head(combinedFile)
print(nrow(combinedFile))

#Columns to scale (excluding the last column)
cols_to_scale <- 1:(ncol(combinedFile) - 1)

# Scale the selected columns
scaled <- combinedFile
scaled[, cols_to_scale] <- scale(combinedFile[, cols_to_scale])

combinedFile = scaled

head(combinedFile)
print(nrow(combinedFile))

######Train classification models for "*Combined Features*"
###

# Split the data into training set and testing set
set.seed(123)
train_indices <- createDataPartition(y = combinedFile$label, p = 0.7, list = FALSE, times = 1)
trainSet <- combinedFile[train_indices, ]
testSet <- combinedFile[-train_indices, ]

###Decision Tree###

#Train Decision Tree model
dt <- rpart(label ~ ., data = trainSet, method = "class")

#Make predictions on test data
predictions <- predict(dt, newdata = testSet, type = "prob")[, 2]

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

#Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

#Plot ROC curve
plot(roc_obj, main = "ROC Curve for Decision Tree", col = "blue")

###Random Forest###

#Train Random Forest model
rf <- randomForest(factor(label) ~ ., data = trainSet, ntree = 100)

#Make predictions on test data
predictions <- predict(rf, newdata = testSet, type = "prob")[, 2]

# Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(testSet$label, predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Random Forest", col = "blue")

###Naive Bayes###

#Train Naive Bayes model
nb_model <- naiveBayes(label ~ ., data = trainSet)

#Make predictions on test data
predictions <- predict(nb_model, newdata = testSet, type = "raw")

#Calculate accuracy
predicted_labels <- ifelse(predictions[,2] >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions[,2])

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Naive Bayes", col = "blue")

###Logistic Regression###

# Train Logistic Regression model
lr <- glm(label ~ ., data = trainSet, family = binomial)

#Make predictions on test data
predictions <- predict(lr, newdata = testSet, type = "response")

#Calculate accuracy
predicted_labels <- ifelse(predictions >= 0.5, 1, 0)
accuracy <- mean(predicted_labels == testSet$label)
cat("Accuracy:", accuracy, "\n")

# Generate ROC curve
roc_obj <- roc(ifelse(testSet$label == 1, TRUE, FALSE), predictions)

# Calculate AUC score
auc_score <- auc(roc_obj)
cat("AUC Score:", auc_score, "\n\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Logistic Regression", col = "blue")
